{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Necessary Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport pickle\nfrom collections import defaultdict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:46:39.669495Z","iopub.execute_input":"2025-05-11T19:46:39.669809Z","iopub.status.idle":"2025-05-11T19:46:39.674275Z","shell.execute_reply.started":"2025-05-11T19:46:39.669784Z","shell.execute_reply":"2025-05-11T19:46:39.673186Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Data Processing","metadata":{}},{"cell_type":"code","source":"def preprocess_dataset(dataset_name, base_path, output_dir):\n    print(f\"\\n=== Processing {dataset_name.upper()} Dataset ===\")\n    sessions, item_counts = load_and_create_sessions(dataset_name, base_path)\n    indexed_sessions, item_mapping = filter_and_index_sessions(sessions, item_counts)\n    train_X, train_Y, test_X, test_Y, train_sessions = split_and_format_sessions(indexed_sessions)\n    save_data(output_dir, train_X, train_Y, test_X, test_Y, train_sessions, item_mapping)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:46:39.675836Z","iopub.execute_input":"2025-05-11T19:46:39.676090Z","iopub.status.idle":"2025-05-11T19:46:39.711640Z","shell.execute_reply.started":"2025-05-11T19:46:39.676056Z","shell.execute_reply":"2025-05-11T19:46:39.710840Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Create Sessions","metadata":{}},{"cell_type":"code","source":"def load_and_create_sessions(dataset_name, base_path, session_timeout_ms=30 * 60 * 1000):\n    if dataset_name == 'diginetica':\n        df = pd.read_csv(os.path.join(base_path, 'train-item-views.csv'), sep=';')\n        df.rename(columns={'sessionId': 'visitorid', 'itemId': 'itemid', 'timeframe': 'timestamp'}, inplace=True)\n\n    elif dataset_name == 'retailrocket':\n        df = pd.read_csv(os.path.join(base_path, 'events.csv'), header=None,\n                         names=['timestamp', 'visitorid', 'event', 'itemid', 'transactionid'])\n        df = df[1:]  # skip header row\n        df = df[df['event'] == 'view']\n\n    else:\n        raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n\n    df['timestamp'] = df['timestamp'].astype(int)\n    df['visitorid'] = df['visitorid'].astype(str)\n    df = df.sort_values(['visitorid', 'timestamp'])\n\n    # 30-minute session timeout\n    session_data = []\n    item_counts = defaultdict(int)\n\n    print(f\"Creating sessions for {dataset_name} using 30-minute timeout...\")\n    for visitor_id, group in df.groupby('visitorid'):\n        group = group.sort_values('timestamp')\n        last_time = group.iloc[0]['timestamp']\n        current_session = []\n\n        for _, row in group.iterrows():\n            if row['timestamp'] - last_time > session_timeout_ms:\n                if len(current_session) > 1:\n                    session_data.append(current_session)\n                    for item in current_session:\n                        item_counts[item] += 1\n                current_session = []\n            current_session.append(int(row['itemid']))\n            last_time = row['timestamp']\n\n        if len(current_session) > 1:\n            session_data.append(current_session)\n            for item in current_session:\n                item_counts[item] += 1\n\n    print(f\"Initial sessions formed: {len(session_data)}\")\n    return session_data, item_counts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:46:39.712579Z","iopub.execute_input":"2025-05-11T19:46:39.712900Z","iopub.status.idle":"2025-05-11T19:46:39.727089Z","shell.execute_reply.started":"2025-05-11T19:46:39.712872Z","shell.execute_reply":"2025-05-11T19:46:39.726313Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Filtering","metadata":{}},{"cell_type":"code","source":"def filter_and_index_sessions(session_data, item_counts, min_item_freq=6):\n    items_to_keep = {item for item, count in item_counts.items() if count >= min_item_freq}\n\n    filtered_sessions = []\n    for session in session_data:\n        filtered = [item for item in session if item in items_to_keep]\n        if len(filtered) > 1:\n            filtered_sessions.append(filtered)\n\n    item_mapping = {item: idx + 1 for idx, item in enumerate(sorted(items_to_keep))}\n    indexed_sessions = [[item_mapping[item] for item in session] for session in filtered_sessions]\n\n    print(f\"Sessions after filtering: {len(filtered_sessions)}\")\n    print(f\"Unique items kept: {len(item_mapping)}\")\n    return indexed_sessions, item_mapping","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:46:39.727888Z","iopub.execute_input":"2025-05-11T19:46:39.728424Z","iopub.status.idle":"2025-05-11T19:46:39.752379Z","shell.execute_reply.started":"2025-05-11T19:46:39.728398Z","shell.execute_reply":"2025-05-11T19:46:39.751345Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Splitting","metadata":{}},{"cell_type":"code","source":"def split_and_format_sessions(indexed_sessions):\n    np.random.seed(42)\n    indices = np.arange(len(indexed_sessions))\n    np.random.shuffle(indices)\n    train_cutoff = int(0.8 * len(indices))\n    train_indices = indices[:train_cutoff]\n    test_indices = indices[train_cutoff:]\n\n    train_sessions = [indexed_sessions[i] for i in train_indices]\n    test_sessions = [indexed_sessions[i] for i in test_indices]\n\n    def create_xy(sessions):\n        X, Y = [], []\n        for session in sessions:\n            for i in range(1, len(session)):\n                X.append(session[:i])\n                Y.append(session[i])\n        return X, Y\n\n    train_X, train_Y = create_xy(train_sessions)\n    test_X, test_Y = create_xy(test_sessions)\n\n    return train_X, train_Y, test_X, test_Y, train_sessions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:46:39.754376Z","iopub.execute_input":"2025-05-11T19:46:39.754623Z","iopub.status.idle":"2025-05-11T19:46:39.775798Z","shell.execute_reply.started":"2025-05-11T19:46:39.754603Z","shell.execute_reply":"2025-05-11T19:46:39.774997Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Save the data","metadata":{}},{"cell_type":"code","source":"def save_data(output_dir, train_X, train_Y, test_X, test_Y, train_sessions, item_mapping):\n    os.makedirs(output_dir, exist_ok=True)\n    with open(os.path.join(output_dir, 'train.txt'), 'wb') as f:\n        pickle.dump((train_X, train_Y), f)\n    with open(os.path.join(output_dir, 'test.txt'), 'wb') as f:\n        pickle.dump((test_X, test_Y), f)\n    with open(os.path.join(output_dir, 'all_train_seq.txt'), 'wb') as f:\n        pickle.dump(train_sessions, f)\n\n    print(f\"Train samples: {len(train_X)} | Test samples: {len(test_X)} | Items: {len(item_mapping)}\")\n    print(f\"Data saved to {output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:46:39.776798Z","iopub.execute_input":"2025-05-11T19:46:39.777055Z","iopub.status.idle":"2025-05-11T19:46:39.793220Z","shell.execute_reply.started":"2025-05-11T19:46:39.777034Z","shell.execute_reply":"2025-05-11T19:46:39.792503Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Run the pipeline","metadata":{}},{"cell_type":"code","source":"preprocess_dataset('diginetica', '/kaggle/input/diginetica-dataset/', './datasets/diginetica/')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:46:39.794041Z","iopub.execute_input":"2025-05-11T19:46:39.794278Z","iopub.status.idle":"2025-05-11T19:49:53.274708Z","shell.execute_reply.started":"2025-05-11T19:46:39.794254Z","shell.execute_reply":"2025-05-11T19:49:53.273962Z"}},"outputs":[{"name":"stdout","text":"\n=== Processing DIGINETICA Dataset ===\nCreating sessions for diginetica using 30-minute timeout...\nInitial sessions formed: 219630\nSessions after filtering: 201580\nUnique items kept: 37866\nTrain samples: 611113 | Test samples: 152576 | Items: 37866\nData saved to ./datasets/diginetica/\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"preprocess_dataset('retailrocket', '/kaggle/input/ecommerce-dataset/', './datasets/retailrocket/')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:49:53.275487Z","iopub.execute_input":"2025-05-11T19:49:53.275711Z","iopub.status.idle":"2025-05-11T20:00:31.336433Z","shell.execute_reply.started":"2025-05-11T19:49:53.275693Z","shell.execute_reply":"2025-05-11T20:00:31.335581Z"}},"outputs":[{"name":"stdout","text":"\n=== Processing RETAILROCKET Dataset ===\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/4230369516.py:7: DtypeWarning: Columns (0,1,3,4) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(os.path.join(base_path, 'events.csv'), header=None,\n","output_type":"stream"},{"name":"stdout","text":"Creating sessions for retailrocket using 30-minute timeout...\nInitial sessions formed: 367652\nSessions after filtering: 297244\nUnique items kept: 42964\nTrain samples: 604751 | Test samples: 149162 | Items: 42964\nData saved to ./datasets/retailrocket/\n","output_type":"stream"}],"execution_count":9}]}